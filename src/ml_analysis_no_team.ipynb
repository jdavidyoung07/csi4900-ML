{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas, numpy, matplotlib and seaborn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# RiotWatcher\n",
    "from riotwatcher import LolWatcher, ApiError\n",
    "\n",
    "# OS tools\n",
    "import os\n",
    "import shutil\n",
    "import json\n",
    "import sys\n",
    "import dill\n",
    "import ipython_genutils\n",
    "\n",
    "# Custom scripts\n",
    "from extract_players_performance import extract_players_performance\n",
    "from remove_perks import remove_perks\n",
    "from cleaner import replace_champ_names_with_tags\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "champ_data = {}\n",
    "with open('../champion_data/champion.json','r',encoding='utf-8') as f :\n",
    "    champions = json.load(f)\n",
    "    for champion in champions['data'] :\n",
    "        champ_data[str.lower(champion)] = champions['data'][champion]\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def add(old,new_df) :\n",
    "    appended_df = pd.concat([old,new_df],ignore_index=True)\n",
    "    return appended_df\n",
    "\n",
    "def champ_name_replacer(champ_name) :\n",
    "    return champ_data[str.lower(champ_name)]['tags'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataframe shape:\n",
      "(74239, 37)\n",
      "\n",
      "dataframe sample:\n",
      "   total_gold_earned_0  total_gold_spent_0  total_baron_kills_0  \\\n",
      "0               102871              112435                    3   \n",
      "\n",
      "   total_dragon_kills_0  total_inhibitor_kils_0  total_kills_0  \\\n",
      "0                     3                       4             50   \n",
      "\n",
      "   total_deaths_0  total_damage_dealt_to_champions_0  \\\n",
      "0              67                             251493   \n",
      "\n",
      "   total_damage_dealt_to_objectives_0  total_damage_taken_0  ...  \\\n",
      "0                              114384                277343  ...   \n",
      "\n",
      "   average_champion_experience_1  gameLengthMin  dmg_to_champs_winner  \\\n",
      "0                          26031              0                     0   \n",
      "\n",
      "   dmg_to_obj_winner  vision_winner  cs_winner  champ_experience_winner  \\\n",
      "0                  0              0          1                        1   \n",
      "\n",
      "   wards_placed_winner  gold_spender_winner  final_match_winner  \n",
      "0                    1                    0                   1  \n",
      "\n",
      "[1 rows x 37 columns]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../ml_data/full_ml_data.csv')\n",
    "columns = list(df.columns)\n",
    "champ_name_columns = [c for c in columns if c.startswith('team_comp') or c.startswith('dmg_carry') or c.startswith('obj_carry')]\n",
    "\n",
    "\n",
    "for c in champ_name_columns :\n",
    "    df[c] = df[c].apply(lambda x : champ_name_replacer(x))\n",
    "\n",
    "# Drop all team composition related rows\n",
    "df = df.drop(['dmg_carry_0', 'obj_carry_0', 'team_comp_0_champ_1', 'team_comp_0_champ_2', 'team_comp_0_champ_3', 'team_comp_0_champ_4', 'team_comp_0_champ_5', 'dmg_carry_1', 'obj_carry_1', 'team_comp_1_champ_1', 'team_comp_1_champ_2', 'team_comp_1_champ_3', 'team_comp_1_champ_4', 'team_comp_1_champ_5'], axis=1)\n",
    "\n",
    "print(f\"dataframe shape:\\n{df.shape}\\n\")\n",
    "print(f\"dataframe sample:\\n{df.head(1)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training data and labels : (49740, 36) (49740,)\n",
      "Shape of test data and labels : (24499, 36) (24499,)\n"
     ]
    }
   ],
   "source": [
    "data = df.drop(\"final_match_winner\",axis=1)\n",
    "labels = df[\"final_match_winner\"].copy()\n",
    "\n",
    "X_train,X_test,y_train,y_test= train_test_split(data,labels,test_size=0.33,random_state=42)\n",
    "\n",
    "print('Shape of training data and labels :',X_train.shape,y_train.shape)\n",
    "print('Shape of test data and labels :',X_test.shape,y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows with missing values:\n",
      " Empty DataFrame\n",
      "Columns: [total_gold_earned_0, total_gold_spent_0, total_baron_kills_0, total_dragon_kills_0, total_inhibitor_kils_0, total_kills_0, total_deaths_0, total_damage_dealt_to_champions_0, total_damage_dealt_to_objectives_0, total_damage_taken_0, average_vision_score_0, total_wards_placed_0, average_creep_score_0, average_champion_experience_0, total_gold_earned_1, total_gold_spent_1, total_baron_kills_1, total_dragon_kills_1, total_inhibitor_kils_1, total_kills_1, total_deaths_1, total_damage_dealt_to_champions_1, total_damage_dealt_to_objectives_1, total_damage_taken_1, average_vision_score_1, total_wards_placed_1, average_creep_score_1, average_champion_experience_1, gameLengthMin, dmg_to_champs_winner, dmg_to_obj_winner, vision_winner, cs_winner, champ_experience_winner, wards_placed_winner, gold_spender_winner]\n",
      "Index: []\n",
      "\n",
      "[0 rows x 36 columns]\n",
      "Missing value row shape: (0, 36)\n"
     ]
    }
   ],
   "source": [
    "missing_value_row =  data[data.isnull().any(axis=1)].head()\n",
    "print(f'Rows with missing values:\\n {missing_value_row}')\n",
    "print(f'Missing value row shape: {missing_value_row.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape (with dummy) : (49740, 36)\n",
      "Columns of training data : Index(['total_gold_earned_0', 'total_gold_spent_0', 'total_baron_kills_0',\n",
      "       'total_dragon_kills_0', 'total_inhibitor_kils_0', 'total_kills_0',\n",
      "       'total_deaths_0', 'total_damage_dealt_to_champions_0',\n",
      "       'total_damage_dealt_to_objectives_0', 'total_damage_taken_0',\n",
      "       'average_vision_score_0', 'total_wards_placed_0',\n",
      "       'average_creep_score_0', 'average_champion_experience_0',\n",
      "       'total_gold_earned_1', 'total_gold_spent_1', 'total_baron_kills_1',\n",
      "       'total_dragon_kills_1', 'total_inhibitor_kils_1', 'total_kills_1',\n",
      "       'total_deaths_1', 'total_damage_dealt_to_champions_1',\n",
      "       'total_damage_dealt_to_objectives_1', 'total_damage_taken_1',\n",
      "       'average_vision_score_1', 'total_wards_placed_1',\n",
      "       'average_creep_score_1', 'average_champion_experience_1',\n",
      "       'gameLengthMin', 'dmg_to_champs_winner', 'dmg_to_obj_winner',\n",
      "       'vision_winner', 'cs_winner', 'champ_experience_winner',\n",
      "       'wards_placed_winner', 'gold_spender_winner'],\n",
      "      dtype='object')\n",
      "One hot encoded Training data shape (without dummy) : (49740, 36)\n"
     ]
    }
   ],
   "source": [
    "print('Training data shape (with dummy) :',X_train.shape)\n",
    "print('Columns of training data :',X_train.columns)\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "    ('std_scaler',StandardScaler())\n",
    "])\n",
    "\n",
    "full_column_set = set(list(X_train.columns))\n",
    "num_columns = list(full_column_set)\n",
    "\n",
    "full_pipeline = ColumnTransformer([\n",
    "        ('num',num_pipeline,num_columns),\n",
    "    ],remainder='passthrough')\n",
    "\n",
    "X_train_prepared = full_pipeline.fit_transform(X_train)\n",
    "size = len(X_train_prepared)\n",
    "X_train_prepared = X_train_prepared[:size]\n",
    "print('Training data shape (without dummy) :',X_train_prepared.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the following four different models with their default hyperparameter values to be trained using the preprocessed data\n",
    "# Support Vector Machine\n",
    "svm_model = SVC()\n",
    "\n",
    "# Decision Trees\n",
    "dt_model = DecisionTreeClassifier()\n",
    "\n",
    "# Random Forests\n",
    "rf_model = RandomForestClassifier()\n",
    "\n",
    "# Naive Bayes\n",
    "nb_model = GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC best params:\n",
      "{'C': 100, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "Decision Tree best params:\n",
      "{'max_depth': 4, 'min_samples_leaf': 20, 'min_samples_split': 5}\n",
      "Random Forest best params:\n",
      "{'bootstrap': True, 'max_depth': 5, 'n_estimators': 200}\n",
      "Naive Bayes best params:\n",
      "{}\n",
      "SVC best estimator:\n",
      "SVC(C=100, gamma=0.001)\n",
      "Decision Tree best estimator:\n",
      "DecisionTreeClassifier(max_depth=4, min_samples_leaf=20, min_samples_split=5)\n",
      "Random Forest best estimator:\n",
      "RandomForestClassifier(max_depth=5, n_estimators=200)\n",
      "Naive Bayes best estimator:\n",
      "GaussianNB()\n",
      "SVC best score:\n",
      "0.9899734548489789\n",
      "Decision Tree best score:\n",
      "0.9828303342650827\n",
      "Random Forest best score:\n",
      "0.9819689028651292\n",
      "Naive Bayes best score:\n",
      "0.9635265742681884\n"
     ]
    }
   ],
   "source": [
    "# use sklearn GridSearchCV to train selected model with hyperparameter tuning\n",
    "# parameters for SVC:\n",
    "    # C -> e.g., 10, 100\n",
    "    # gamma ->  e.g., 0.001, 0.0001\n",
    "    # kernel -> 'rbf' or 'linear' \n",
    "\n",
    "svm_params = [\n",
    "    {'C':[10,100],'gamma':[0.001,0.0001],'kernel':['rbf','linear']}\n",
    "]\n",
    "\n",
    "# parameters for DecisionTreeClassifier: \n",
    "    # max_depth ->  e.g., 3, 4\n",
    "    # min_samples_split -> 5, 10\n",
    "    # min_samples_leaf -> 10, 20\n",
    "dt_params = [\n",
    "    {'max_depth':[3,4],'min_samples_split':[5,10],'min_samples_leaf':[10,20]}\n",
    "]\n",
    "\n",
    "# parameters for RandomForestClassifier: \n",
    "    # n_estimators -> 100, 200\n",
    "    # max_depth -> 3, 5\n",
    "    # bootstrap -> True, False\n",
    "rf_params = [\n",
    "    {'n_estimators':[100,200],'max_depth':[3,5],'bootstrap':[True,False]}\n",
    "]\n",
    "\n",
    "nb_params = [{}]\n",
    "# initialize gridsearch with the required parameters, including the following scoring methods and refit='bal_accuracy' (2)\n",
    "scoring = {\"accuracy\": \"accuracy\", \"bal_accuracy\": \"balanced_accuracy\", \"F1_macro\": \"f1_macro\"}\n",
    "grid_search_svc = GridSearchCV(svm_model,svm_params,scoring=scoring,cv=5,refit='bal_accuracy' ,return_train_score=True)\n",
    "grid_search_dt = GridSearchCV(dt_model,dt_params,scoring=scoring,cv=5,refit='bal_accuracy' ,return_train_score=True)\n",
    "grid_search_rf = GridSearchCV(rf_model,rf_params,scoring=scoring,cv=5,refit='bal_accuracy' ,return_train_score=True)\n",
    "grid_search_nb = GridSearchCV(nb_model,nb_params,scoring=scoring,cv=5,refit='bal_accuracy' ,return_train_score=True)\n",
    "\n",
    "# fit the training data (0.5)\n",
    "grid_search_svc.fit(X_train_prepared,y_train)\n",
    "grid_search_dt.fit(X_train_prepared,y_train)\n",
    "grid_search_rf.fit(X_train_prepared,y_train)\n",
    "grid_search_nb.fit(X_train_prepared,y_train)\n",
    "\n",
    "# print the best parameters (0.5)\n",
    "print(f'SVC best params:\\n{grid_search_svc.best_params_}')\n",
    "print(f'Decision Tree best params:\\n{grid_search_dt.best_params_}')\n",
    "print(f'Random Forest best params:\\n{grid_search_rf.best_params_}')\n",
    "print(f'Naive Bayes best params:\\n{grid_search_nb.best_params_}')\n",
    "\n",
    "# print the best estimator (0.5)\n",
    "print(f'SVC best estimator:\\n{grid_search_svc.best_estimator_}')\n",
    "print(f'Decision Tree best estimator:\\n{grid_search_dt.best_estimator_}')\n",
    "print(f'Random Forest best estimator:\\n{grid_search_rf.best_estimator_}')\n",
    "print(f'Naive Bayes best estimator:\\n{grid_search_nb.best_estimator_}')\n",
    "\n",
    "# print the best score from trained GridSearchCV model (0.5)\n",
    "print(f'SVC best score:\\n{grid_search_svc.best_score_}')\n",
    "print(f'Decision Tree best score:\\n{grid_search_dt.best_score_}')\n",
    "print(f'Random Forest best score:\\n{grid_search_rf.best_score_}')\n",
    "print(f'Naive Bayes best score:\\n{grid_search_nb.best_score_}')\n",
    "\n",
    "# Save session to \"notebook_env.db\"\n",
    "dill.dump_session(\"no_teamcomp_notebook_env.db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape (with dummy) : (24499, 36)\n",
      "Columns of training data : Index(['total_gold_earned_0', 'total_gold_spent_0', 'total_baron_kills_0',\n",
      "       'total_dragon_kills_0', 'total_inhibitor_kils_0', 'total_kills_0',\n",
      "       'total_deaths_0', 'total_damage_dealt_to_champions_0',\n",
      "       'total_damage_dealt_to_objectives_0', 'total_damage_taken_0',\n",
      "       'average_vision_score_0', 'total_wards_placed_0',\n",
      "       'average_creep_score_0', 'average_champion_experience_0',\n",
      "       'total_gold_earned_1', 'total_gold_spent_1', 'total_baron_kills_1',\n",
      "       'total_dragon_kills_1', 'total_inhibitor_kils_1', 'total_kills_1',\n",
      "       'total_deaths_1', 'total_damage_dealt_to_champions_1',\n",
      "       'total_damage_dealt_to_objectives_1', 'total_damage_taken_1',\n",
      "       'average_vision_score_1', 'total_wards_placed_1',\n",
      "       'average_creep_score_1', 'average_champion_experience_1',\n",
      "       'gameLengthMin', 'dmg_to_champs_winner', 'dmg_to_obj_winner',\n",
      "       'vision_winner', 'cs_winner', 'champ_experience_winner',\n",
      "       'wards_placed_winner', 'gold_spender_winner'],\n",
      "      dtype='object')\n",
      "Testing data shape (without dummy) : (49740, 36)\n"
     ]
    }
   ],
   "source": [
    "# Prepare X_test dataset based on previous method for X_train\n",
    "print('Training data shape (with dummy) :',X_test.shape)\n",
    "print('Columns of training data :',X_test.columns)\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "    ('std_scaler',StandardScaler())\n",
    "])\n",
    "\n",
    "full_column_set = set(list(X_test.columns))\n",
    "num_columns = list(full_column_set)\n",
    "\n",
    "full_pipeline = ColumnTransformer([\n",
    "        ('num',num_pipeline,num_columns),\n",
    "    ],remainder='passthrough')\n",
    "\n",
    "X_test_prepared = full_pipeline.fit_transform(X_test)\n",
    "size = len(X_test_prepared)\n",
    "X_test_prepared = X_test_prepared[:size]\n",
    "print('Testing data shape (without dummy) :',X_train_prepared.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing models with test data\n",
    "\n",
    "# Using the following existing variables:\n",
    "# X_test_prepared: test data\n",
    "# y_test: test labels\n",
    "# Models:\n",
    "# grid_search_svc\n",
    "# grid_search_dt\n",
    "# grid_search_rf\n",
    "# grid_search_nb\n",
    "\n",
    "# Predict using models' best estimators\n",
    "prediction_svc = grid_search_svc.best_estimator_.predict(X_test_prepared)\n",
    "prediction_dt = grid_search_dt.best_estimator_.predict(X_test_prepared)\n",
    "prediction_rf = grid_search_rf.best_estimator_.predict(X_test_prepared)\n",
    "prediction_nb = grid_search_nb.best_estimator_.predict(X_test_prepared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix: SVC:\n",
      "[[12456   116]\n",
      " [   97 11830]]\n",
      "\n",
      "Confusion matrix: DT:\n",
      "[[12363   209]\n",
      " [  175 11752]]\n",
      "\n",
      "Confusion matrix: RF:\n",
      "[[12349   223]\n",
      " [  192 11735]]\n",
      "\n",
      "Confusion matrix: NB:\n",
      "[[12108   464]\n",
      " [  407 11520]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create and print classification reports for all models\n",
    "\n",
    "# SVC\n",
    "conf_matrix_svc = confusion_matrix(y_test, prediction_svc)\n",
    "class_report_svc = classification_report(y_test, prediction_svc, output_dict=True)\n",
    "print(f'Confusion matrix: SVC:\\n{conf_matrix_svc}\\n')\n",
    "\n",
    "# Decision Tree\n",
    "conf_matrix_dt = confusion_matrix(y_test, prediction_dt)\n",
    "class_report_dt = classification_report(y_test, prediction_dt, output_dict=True)\n",
    "print(f'Confusion matrix: DT:\\n{conf_matrix_dt}\\n')\n",
    "\n",
    "# Random Forest\n",
    "conf_matrix_rf = confusion_matrix(y_test, prediction_rf)\n",
    "class_report_rf = classification_report(y_test, prediction_rf, output_dict=True)\n",
    "print(f'Confusion matrix: RF:\\n{conf_matrix_rf}\\n')\n",
    "\n",
    "# Naive Bayes\n",
    "conf_matrix_nb = confusion_matrix(y_test, prediction_nb)\n",
    "class_report_nb = classification_report(y_test, prediction_nb, output_dict=True)\n",
    "print(f'Confusion matrix: NB:\\n{conf_matrix_nb}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score       support\n",
      "accuracy       0.991306  0.991306  0.991306      0.991306\n",
      "macro avg      0.991281  0.991320  0.991300  24499.000000\n",
      "weighted avg   0.991307  0.991306  0.991306  24499.000000\n",
      "\n",
      "              precision    recall  f1-score       support\n",
      "accuracy       0.984326  0.984326  0.984326      0.984326\n",
      "macro avg      0.984284  0.984352  0.984316  24499.000000\n",
      "weighted avg   0.984331  0.984326  0.984326  24499.000000\n",
      "\n",
      "              precision    recall  f1-score       support\n",
      "accuracy       0.983061  0.983061  0.983061      0.983061\n",
      "macro avg      0.983021  0.983082  0.983050  24499.000000\n",
      "weighted avg   0.983065  0.983061  0.983061  24499.000000\n",
      "\n",
      "              precision    recall  f1-score       support\n",
      "accuracy       0.964448  0.964448  0.964448      0.964448\n",
      "macro avg      0.964380  0.964484  0.964427  24499.000000\n",
      "weighted avg   0.964462  0.964448  0.964450  24499.000000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print our classification reports\n",
    "\n",
    "# SVC\n",
    "df_svc = pd.DataFrame(class_report_svc).transpose().drop(['0', '1'])\n",
    "print(f\"{df_svc}\\n\")\n",
    "\n",
    "# DT\n",
    "df_dt = pd.DataFrame(class_report_dt).transpose().drop(['0', '1'])\n",
    "print(f\"{df_dt}\\n\")\n",
    "\n",
    "# RF\n",
    "df_rf = pd.DataFrame(class_report_rf).transpose().drop(['0', '1'])\n",
    "print(f\"{df_rf}\\n\")\n",
    "\n",
    "# NB\n",
    "df_nb = pd.DataFrame(class_report_nb).transpose().drop(['0', '1'])\n",
    "print(f\"{df_nb}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save session to \"notebook_env.db\"\n",
    "dill.dump_session(\"no_teamcomp_notebook_env.db\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('honours')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b7b7b3ece1175e03f6d4ad1d6f449bb296e00ba8bb500d8c0af43823720e462e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
